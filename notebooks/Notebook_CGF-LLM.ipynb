{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Causal Graph Fuzzy LLM (MISO)"
      ],
      "metadata": {
        "id": "ueZ_XaXk3Ep2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgjDjdSx3Bde"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet git+https://github.com/PatriciaLucas/AutoML.git\n",
        "!pip install --quiet git+https://github.com/petroniocandido/clshq_tk.git\n",
        "!git clone https://github.com/FutureLab-DCC/CGF-LLM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import sys\n",
        "sys.path.append('CGF-LLM')\n",
        "import CGFLLM, utils"
      ],
      "metadata": {
        "id": "2m8bZ1rh5Tds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_dataset = 'SONDA'\n",
        "target = 'glo_avg'\n",
        "df = utils.get_dataset(name = name_dataset)\n",
        "\n",
        "max_lags = 20\n",
        "partitions = 30\n",
        "epochs = 20\n",
        "database_metrics_path = 'CGFLLM_metrics.db'\n",
        "database_size_path = 'CGFLLM_size.db'\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "name_model = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_model)\n",
        "\n",
        "windows = utils.rolling_window(df, 10)\n",
        "\n",
        "utils.execute(\"CREATE TABLE IF NOT EXISTS results(model TEXT, name_dataset TEXT, window INT, predict FLOAT, \\\n",
        "               real FLOAT)\", database_metrics_path)\n",
        "utils.execute(\"CREATE TABLE IF NOT EXISTS results(model TEXT, name_dataset TEXT, window INT, total_chars FLOAT, train_token_count FLOAT, test_token_count FLOAT, train_char_count FLOAT, \\\n",
        "               test_char_count FLOAT)\", database_size_path)"
      ],
      "metadata": {
        "id": "UJkb13Rb5fzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the windows\n",
        "for i, window in enumerate(windows):\n",
        "      if i < 1:  # Enter the number of the last window saved in the database here\n",
        "        print(\"window already executed\")\n",
        "\n",
        "      else:\n",
        "\n",
        "        # Exclude constant series.\n",
        "        for variable in window.columns:\n",
        "            if window[variable].max() == window[variable].min():\n",
        "                window = window.drop(variable, axis=1)\n",
        "                print(f\"Variables {variable} were deleted because they are constant.\")\n",
        "\n",
        "        # CAUSAL GRAPH FUZZY\n",
        "        # model_name = 'CGF-LLM'\n",
        "        # ds, scaler, tokenizer, seq, graph = CGFLLM.fuzzy_causal(window, name_dataset, target, max_lags, tokenizer, partitions)\n",
        "\n",
        "        # TEXT\n",
        "        # model_name = 'TEXT-LLM'\n",
        "        # ds, scaler, tokenizer, seq, graph = CGFLLM.text(window, name_dataset, target, max_lags, tokenizer)\n",
        "\n",
        "        # CAUSAL GRAPH TEXT\n",
        "        # model_name = 'CGTEXT-LLM'\n",
        "        # ds, scaler, tokenizer, seq, graph = CGFLLM.causal_text(window, name_dataset, target, max_lags, tokenizer)\n",
        "\n",
        "        train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "        freeze = True # Freeze GPT-2 weights\n",
        "        output_size = 1\n",
        "        model = CGFLLM.train_model(train_dataset, name_model, epochs, scaler, output_size, freeze)\n",
        "\n",
        "        forecasts, real = CGFLLM.predict(test_dataset, model, tokenizer, target, scaler)\n",
        "\n",
        "        for row in range(len(forecasts)):\n",
        "            utils.execute_insert(\"INSERT INTO results VALUES(?, ?, ?, ?, ?)\", (model_name, name_dataset, i, float(forecasts[row]), float(real[row])), database_metrics_path)\n",
        "\n",
        "        # Calculates number of tokens and text\n",
        "        total_chars, train_token_count, test_token_count, train_char_count, test_char_count = utils.calculate_train_test_sizes(ds, seq, tokenizer)\n",
        "        utils.execute_insert(\"INSERT INTO results VALUES(?, ?, ?, ?, ?, ?, ?, ?)\", (model_name, name_dataset, i, total_chars, train_token_count, test_token_count, train_char_count, test_char_count), database_size_path)\n",
        "\n",
        "        print(f'Save window: {i}')"
      ],
      "metadata": {
        "id": "RZDR95yz6CfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to view the results"
      ],
      "metadata": {
        "id": "gXzAPv-bkm3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_metrics = utils.get_metrics(database_metrics_path)\n",
        "avg_metrics"
      ],
      "metadata": {
        "id": "qp7Oy0Yd7PSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sizes = utils.get_sizes(database_size_path)\n",
        "avg_sizes"
      ],
      "metadata": {
        "id": "wRM4x0cwAh2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}